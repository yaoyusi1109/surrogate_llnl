---
title: "ACT Progress Report: Year 1 Quarter 2"
date: "August 15, 2025 - December 15, 2025"
author: "Annie Booth"
bibliography: ref.bib 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

### Abstract

### Summary of Objectives

Objective 1: Provide research plans to LLNL and suggestions for each quarters activities to be approved by LLNS.

Objective 2: Perform methodological development of Bayesian DGP emulator models for variable selection and learning of lower dimensional manifolds.

### Publications

There are no publications of this work yet.

******

# Technical Details

```{r}
library(deepgp)
library(mvtnorm)
library(lhs)
library(interp)
library(invgamma)
```

Running simulations with our previous method, I encountered many issues.

* I implemented estimated of both tau2 and theta on latent layers.  It was very hard
to figure out a good fixed theta value.  Not sure if estimating both is the right thing to do just yet though.  It still contributes to flat likelihoods.
* Occasionally the scale estimates would run to infinity.  I "band-aid" fixed this
by setting an upper bound of 1.
* Occasionally the scale estimates would run to zero.  The predictions were working fine, there was just no control on the scale.  Everything shrunk together, so the fits were the same.  But not pretty or useful for thresholding the parameters.  Couldn't seem to adjust this with priors - the prior did not have enough control.

Then - viola!  I figured out a new and better way to monotonically warp samples from the GP prior.  Let's investigate.

OLD way of monotonically warping:

```{r}
x <- matrix(seq(0, 1, length = 100), ncol = 1)
dx <- sq_dist(x)
x_grid <- matrix(seq(0, 1, length = 50), ncol = 1)
dx_grid <- sq_dist(x_grid)
grid_index <- deepgp:::fo_approx_init(x_grid, x)
```

```{r, fig.width = 10, fig.height = 3}
r <- 15
for (tau2 in c(0.01, 0.1, 1, 10)) {
  par(mfrow = c(1, 4), mar = c(4, 4, 4, 1))
  for (theta in c(0.01, 0.1, 1, 10)) {
    w_grid <- t(rmvnorm(r, sigma = tau2*exp(-dx_grid/theta)))
    w <- matrix(nrow = nrow(x), ncol = r)
    for (i in 1:r) {
      w[, i] <- deepgp:::monowarp_ref(x, x_grid, w_grid[, i], grid_index)
      w[, i] <- (w[, i] - mean(w[, i]))
    }
    matplot(x, w, type = "l", main = paste0("tau2 = ", tau2, "\n theta = ", theta,
                                            "\n ratio = ", tau2/theta))
  }
}
```

NEW way of monotonically warping:

```{r, fig.width = 10, fig.height = 3}
r <- 15
for (tau2 in c(0.01, 0.1, 1, 10)) {
  par(mfrow = c(1, 4), mar = c(4, 4, 4, 1))
  for (theta in c(0.01, 0.1, 1, 10)) {
    wo <- t(rmvnorm(r, sigma = tau2*exp(-dx/theta)))
    w <- matrix(nrow = nrow(x), ncol = r)
    for (i in 1:r) {
      w[, i] <- c(0, cumsum(abs(diff(wo[, i]))))
      w[, i] <- (w[, i] - mean(w[, i]))
    }
    matplot(x, w, type = "l", main = paste0("tau2 = ", tau2, "\n theta = ", theta,
                                            "\n ratio = ", tau2/theta))
  }
}
```

Smaller theta = more linear.  Smaller tau2 = more flat.

Even better?... it looks like we can fix tau2 at 1 and let theta do all the work???

## Updated monowarp DGP model

A standard DGP:

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 (K_{\theta_w}(W) + g\mathbb{I})\right) \\
\mathbf{w}_p &\sim\mathrm{GP}\left(0,\; K_{\theta_x^p}(X) \right) \quad p=1,\dots,P
\end{array}
\quad\textrm{where}\quad
W = \begin{bmatrix} \mathbf{w}_1 & \mathbf{w}_2 & \dots & \mathbf{w}_p\end{bmatrix}
$$

A monowarp DGP uses a transformation $T(\mathbf{w}_p)$ which takes $\mathbf{w}_p$ and makes it monotonic.

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 (K_{\theta_w}(W_\textrm{mono}) + g\mathbb{I})\right) \\
\mathbf{w}_p &\sim\mathrm{GP}\left(0,\; K_{\theta_x^p}(X) \right) \quad p=1,\dots,P
\end{array}
\quad\textrm{where}\quad
W = \begin{bmatrix} T(\mathbf{w}_1) & T(\mathbf{w}_2) & \dots & T(\mathbf{w}_p)\end{bmatrix}
$$

Previously, we were using the monotonic warping function from @barnett2025monotonic.  This involves a reference process, with exponentiaion, then a cumulative sum, then post-processing to [0, 1].  In our tests, we found this to be a very finicky process.  It wasn't very numerically stable.  We need something a bit better.

I propose a new function $T(\cdot)$ which offers a better warping, preserving the main structure of $\mathbf{w}_i$.

$$
T(\mathbf{w}) = \mathrm{cumsum}(\mathrm{abs}(\mathrm{diff}(\mathbf{w}_i))
$$

Let's see this in action.

```{r}
theta <- 0.01
tau2 <- 1

par(mfcol = c(2, 4))
for (n in c(5, 10, 50, 100)) {
  x <- seq(0, 1, length = n)
  K <- exp(-sq_dist(x)/theta)
  w <- drop(rmvnorm(1, sigma = K))
  wwarp <- c(0, cumsum(abs(diff(w))))
  wwarp <- wwarp - mean(wwarp)

  plot(x, w, type = "b", main = paste0("Original n = ", n))
  plot(x, wwarp, type = "b", main = paste0("Monowarped n = ", n))
}
```

This gives us exactly what we want.  We can conduct training by simply warping the samples before they are plugged into the likelihood.

## Higher dimension?

If we get away from the reference process, we could 

But how will we predict at a new batch of locations?  We could use a simple linear interpolation or kriging?

```{r}
# Say we have a particular x, w, and wwarp
# And we wanted to get the monotonic warped value at a new xstar location
x <- sort(runif(10, 0.1, 0.9))

K <- exp(-sq_dist(x)/theta)
w <- drop(rmvnorm(1, sigma = K))
wwarp <- c(0, cumsum(abs(diff(w))))
wwarp <- wwarp # - mean(wwarp)

plot(x, wwarp, type = "b")
```

I see a couple options:

* Since we are in one dimension, we could do a simple linear interpolation.  But this would not work well for predictive values that are outside the range of the training data.  (But extrapolating is never really good...)  This is similar to the original implementation, but the reference grid made sure there was always a point at the edges.
* We could use the kriging predictor to get a good prediction of xstar to the unwarped w (this is just a GP with a lengthscale that we know).  But then could we still do a cumulative sum?  If we are smart we could?!

```{r}
xstar <- seq(0, 1, length = 100)
wstar <- deepgp:::krig(w, sq_dist(x), sq_dist(xstar), sq_dist(xstar, x),
                   tau2 = 1, theta = theta, g = 1e-8)$mean

wwarp_star <- vector(length = 100)
for (i in 1:length(xstar)) {
  w_combo <- c(w[x < xstar[i]], wstar[i])
  wwarp_star[i] <- sum(abs(diff(w_combo)))
}

par(mfrow = c(1, 2))
plot(x, w, type = "b", main = "Original space, kriging map")
lines(xstar, wstar, col = 2, pch = 17)
plot(x, wwarp, type = "b", main = "Warped space, linear map")
lines(xstar, wwarp_star, col = 2, pch = 17)
```

This idea doesn't work, because things are not monotonic.  This was exactly the reasoning for the reference grid.  

Similarly, if we tried to krig directly to the warped space, this wouldn't enforce monotonicity.  And we wouldn't know the theta/tau2 to use.

Let's do the easy thing - linear mappings.

First, how did we previously deal with predictive locations that were outside of the training data?  This is inside the `fo_approx` function. The reference process anchors the edges of the space.

```{r}
xnew <- as.matrix(seq(0.1, 0.9, length = 10))
xgrid <- as.matrix(seq(0, 1, length = 50))
wgrid <- drop(rmvnorm(1, sigma = exp(-sq_dist(xgrid)/0.1)))
grid_index <- deepgp:::fo_approx_init(xgrid, xgrid)
wwarp <- deepgp:::monowarp_ref(xgrid, xgrid, wgrid, grid_index)
grid_index_new <- deepgp:::fo_approx_init(xgrid, xnew)
wnew <- deepgp:::monowarp_ref(xnew, xgrid, wgrid, grid_index_new)

plot(xgrid, wwarp, type = "b")
points(xnew, wnew, col = 2, pch = 17)
```

Would it have been ok if we skipped over it?  Yes, but the warping looks totally different.

```{r}
x <- as.matrix(sort(runif(20)))
w <- drop(rmvnorm(1, sigma = exp(-sq_dist(x)/0.1)))
xnew <- as.matrix(seq(0, 1, length = 10))
grid_index <- deepgp:::fo_approx_init(x, x)
wwarp <- deepgp:::monowarp_ref(x, x, w, grid_index)
grid_index_new <- deepgp:::fo_approx_init(x, xnew)
wnew <- deepgp:::monowarp_ref(xnew, x, w, grid_index_new)

plot(x, wwarp, type = "b", xlim = c(0, 1))
points(xnew, wnew, col = 2, pch = 17)
```

Can we do this in higher dimensions?

```{r}
x <- randomLHS(50, 2)
w <- rmvnorm(1, sigma = exp(-sq_dist(x)/0.1))

# Order by distance from the origin (x min values must be zero)
origin <- matrix(c(0, 0), nrow = 1)
d <- drop(sq_dist(origin, x))
ord <- order(d, decreasing = FALSE)
xord <- x[ord, ]
wwarp <- c(0, cumsum(abs(diff(w[ord]))))

par(mfrow = c(2, 2))
image(interp(x[, 1], x[, 2], w))
persp(interp(x[, 1], x[, 2], w), theta = 30, r = 30, phi = 30)
image(interp(xord[, 1], xord[, 2], wwarp))
persp(interp(xord[, 1], xord[, 2], wwarp), theta = 30, r = 30, phi = 30)
```

Yes, it works!  A reference process in higher dimensions is too much, but we can simply use the training data locations.  Then to predict, we do a linear interploation.

How does linear interpolation work in higher dimension?  We could use the Delaunay triangulation, then use linear interpolation of each triangle containing a predictive location.  In two dimensions, this is known as bilateral interpolation.  There are several R packages that offer this functionality.  In three dimensions, this is known as trilateral interpolation.  I have been unable to find any existing code for that.

The general idea is that the interpolated y value at a point in a "triangle" is a weighted average
of the values from the "triangle's" vertices (here I am using the term loosely since triangles are only the two dimensional form).

Key things I've learned:

* We can do a Delaunay triangulation with the `delaunayn` function in the `geometry` package.
* For a new location, we can identify the "triangle" that contains it with the `tsearchn` function.
* Then, how can we get the proper weights?  Barycentric coordinates...
* There is no clear way to extrapolate outside the convex hull.  I thus propose adding all corner points to
our proposals (even though they will not feature in the likelihood).  This will only get clunky in higher dimensions.

Example of adding corner points and finding the containing elements:

```{r}
library(geometry)

get_corners <- function(d) {
  bounds <- rep(list(c(0, 1)), times = d)
  return(as.matrix(expand.grid(bounds)))
}

d <- 2
x <- randomLHS(10, d)
x <- rbind(x, get_corners(d))
y <- x[, 1] + x[, 2]
xstar <- matrix(runif(d), nrow = 1)

# Triangulation
tri <- delaunayn(x, options = "Q12")

# Identify container
container <- tsearchn(x, tri, xstar)
xvertices <- x[tri[container$idx, ], ]
yvertices <- y[tri[container$idx, ]]
```

Now, how do we get the interpolation?

```{r}
# 2d implementation from interp package
if (d == 2) interp::interp(xvertices[, 1], xvertices[, 2], yvertices,
               xo = xstar[1], yo = xstar[2])$z

# My own way using barycentric coordinates (it matches for d = 2)
sum(cart2bary(xvertices, xstar) * yvertices)
xstar[1] + xstar[2]
```

```{r}
ord_from_origin <- function(x) {
  if (!is.matrix(x)) x <- as.matrix(x)
  d <- ncol(x)
  origin <- matrix(rep(0, times = d), nrow = 1)
  dist_from_origin <- drop(sq_dist(origin, x))
  return(order(dist_from_origin, decreasing = FALSE))
}

monotransform <- function(x, w_orig, ord = NULL) {
  if (!is.matrix(x)) x <- as.matrix(x)
  if (!is.matrix(w_orig)) w_orig <- as.matrix(w_orig)
  if (is.null(ord)) ord <- ord_from_origin(x)
  D <- ncol(w_orig)
  w <- matrix(nrow = nrow(w_orig), ncol = D)
  for (i in 1:D) {
    wwarp <- c(0, cumsum(abs(diff(w_orig[ord, i]))))
    w[, i] <- wwarp[order(ord)]
  }
  return(w)
}

d <- 2
x <- randomLHS(50, d)
bounds <- rep(list(c(0, 1), times = d))
corners <- as.matrix(expand.grid(bounds))
x <- rbind(x, corners)
w_orig <- drop(rmvnorm(1, sigma = exp(-sq_dist(x)/0.1)))
w <- monotransform(x, w_orig)

par(mfrow = c(1, 2))
image(interp(x[, 1], x[, 2], w_orig))
image(interp(x[, 1], x[, 2], w))
```

