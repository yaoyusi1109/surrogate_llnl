---
title: "ACT Progress Report: Year 1 Quarter 2"
date: "August 15, 2025 - November 15, 2025"
author: "Annie Booth"
bibliography: ref.bib 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", eval = FALSE)
```

### Abstract

In the second quarter of Year 1, PI Annie Booth (Virginia Tech) continued working collaboratively with Kevin Quinlan and Laura Wendelberger (LLNL) to develop methodology for deep Gaussian process (DGP) surrogates with large input dimensions.  The entire three-long project spans two directives towards this end: variable selection and dimension reduction.  In this quarter, our team continued focusing on the variable selection problem, building upon the progress made in Quarter 1.  Specifically, we developed new statistical methodology involving a novel Bayesian hierarchical modeling structure and warping function that encourages "deselection" of unimportant variables.  The methodological developments solved the modeling limitations encountered in the work during Quarter 1.  Software was developed in the form of an R-package, and new methods were validated on a variety of synthetic benchmarks, showing excellent performance.  PI Booth was not supported financially this quarter, but she has successfully identified a Virginia Tech student to support on this project in Quarter 3.

### Summary of Objectives

Objective 1: Provide research plans to LLNL and suggestions for each quarters activities to be approved by LLNS.

* At the start of this quarter, PI Booth worked with the LLNL team to decide on the quarter's activities.  The team decided to continue the focus on method development for variable selection with DGPs.  This focus worked out well as the team was able to develop effective methodology/software and test its performance.
* For next quarter:
  + PI Booth will work with Kevin to deploy the developed software on our motiviating application.
  + PI Booth will work with Kevin and Laura to write a paper on the newly developed methods and submit it to a peer-reviewed journal.
  + PI Booth has identified a Virginia Tech student to begin working on this project in January 2026.  The new student will work closely with PI Booth to start tackling the DGP dimension reduction problem.

Objective 2: Perform methodological development of Bayesian DGP emulator models for variable selection and learning of lower dimensional manifolds.

* The team successfully developed statistical methodology and software for variable selection with DGP surrogates.  Technical details and results are provided below.

### Publications

There are no publications of this work yet.

******

# Technical Details

```{r}
library(deepgp)
library(mvtnorm)
library(lhs)
library(interp)
library(invgamma)
```

Running simulations with our previous method, I encountered many issues.

* I implemented estimated of both tau2 and theta on latent layers.  It was very hard
to figure out a good fixed theta value.  Not sure if estimating both is the right thing to do just yet though.  It still contributes to flat likelihoods.
* Occasionally the scale estimates would run to infinity.  I "band-aid" fixed this
by setting an upper bound of 1.
* Occasionally the scale estimates would run to zero.  The predictions were working fine, there was just no control on the scale.  Everything shrunk together, so the fits were the same.  But not pretty or useful for thresholding the parameters.  Couldn't seem to adjust this with priors - the prior did not have enough control.

Then - viola!  I figured out a new and better way to monotonically warp samples from the GP prior.  Let's investigate.

NEW way of monotonically warping:

```{r, fig.width = 10, fig.height = 3}
n <- 10
r <- 5
tau2 <- 0.1
x <- matrix(seq(0, 1, length = 100), ncol = 1)
dx <- sq_dist(x)
par(mfcol = c(2, 4), mar = c(4, 4, 4, 1))
for (theta in c(0.01, 0.1, 1, 10)) {
  w_orig <- t(rmvnorm(r, sigma = tau2*exp(-dx/theta)))
  w <- matrix(nrow = nrow(x), ncol = r)
  for (i in 1:r) {
    w[, i] <- c(0, cumsum(abs(diff(w_orig[, i]))))
  }
  matplot(x, w_orig, type = "l", main = paste0("Orig theta = ", theta))
  matplot(x, w, type = "l", main = paste0("Warped theta = ", theta))
}
```

Crucially, the number of observations (`n`) does not seem to affect the scale.

The problem I see here is that theta is pulling "double duty."  It is controlling how wiggly the warping is AND the scale.  This would be fine if we post-processed all of these to the same scale, but then we would need separable lengthscales on the outer layer.  I think a better fix is to estimate the tau2 on the latent layer (since we know we can integrate it out in a Bayesian manner).  Let's test it.

TODO: implement this in deepgp package

## Updated monowarp DGP model

A standard DGP:

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 (K_{\theta_w}(W) + g\mathbb{I})\right) \\
\mathbf{w}_p &\sim\mathrm{GP}\left(0,\; K_{\theta_x^p}(X) \right) \quad p=1,\dots,P
\end{array}
\quad\textrm{where}\quad
W = \begin{bmatrix} \mathbf{w}_1 & \mathbf{w}_2 & \dots & \mathbf{w}_p\end{bmatrix}
$$

A monowarp DGP uses a transformation $T(\mathbf{w}_p)$ which takes $\mathbf{w}_p$ and makes it monotonic.

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 (K_{\theta_w}(W_\textrm{mono}) + g\mathbb{I})\right) \\
\mathbf{w}_p &\sim\mathrm{GP}\left(0,\; K_{\theta_x^p}(X) \right) \quad p=1,\dots,P
\end{array}
\quad\textrm{where}\quad
W = \begin{bmatrix} T(\mathbf{w}_1) & T(\mathbf{w}_2) & \dots & T(\mathbf{w}_p)\end{bmatrix}
$$

Previously, we were using the monotonic warping function from @barnett2025monotonic.  This involves a reference process, with exponentiaion, then a cumulative sum, then post-processing to [0, 1].  In our tests, we found this to be a very finicky process.  It wasn't very numerically stable.  We need something a bit better.

I propose a new function $T(\cdot)$ which offers a better warping, preserving the main structure of $\mathbf{w}_i$.

$$
T(\mathbf{w}) = \mathrm{cumsum}(\mathrm{abs}(\mathrm{diff}(\mathbf{w}_i))
$$

Let's see this in action.

```{r}
theta <- 0.01
tau2 <- 1

par(mfcol = c(2, 4))
for (n in c(5, 10, 50, 100)) {
  x <- seq(0, 1, length = n)
  K <- exp(-sq_dist(x)/theta)
  w <- drop(rmvnorm(1, sigma = K))
  wwarp <- c(0, cumsum(abs(diff(w))))
  wwarp <- wwarp - mean(wwarp)

  plot(x, w, type = "b", main = paste0("Original n = ", n))
  plot(x, wwarp, type = "b", main = paste0("Monowarped n = ", n))
}
```

This gives us exactly what we want.  We can conduct training by simply warping the samples before they are plugged into the likelihood.

## Higher dimension?

If we get away from the reference process, we could 

But how will we predict at a new batch of locations?  We could use a simple linear interpolation or kriging?

```{r}
# Say we have a particular x, w, and wwarp
# And we wanted to get the monotonic warped value at a new xstar location
x <- sort(runif(10, 0.1, 0.9))

K <- exp(-sq_dist(x)/theta)
w <- drop(rmvnorm(1, sigma = K))
wwarp <- c(0, cumsum(abs(diff(w))))
wwarp <- wwarp # - mean(wwarp)

plot(x, wwarp, type = "b")
```

I see a couple options:

* Since we are in one dimension, we could do a simple linear interpolation.  But this would not work well for predictive values that are outside the range of the training data.  (But extrapolating is never really good...)  This is similar to the original implementation, but the reference grid made sure there was always a point at the edges.
* We could use the kriging predictor to get a good prediction of xstar to the unwarped w (this is just a GP with a lengthscale that we know).  But then could we still do a cumulative sum?  If we are smart we could?!

```{r}
xstar <- seq(0, 1, length = 100)
wstar <- deepgp:::krig(w, sq_dist(x), sq_dist(xstar), sq_dist(xstar, x),
                   tau2 = 1, theta = theta, g = 1e-8)$mean

wwarp_star <- vector(length = 100)
for (i in 1:length(xstar)) {
  w_combo <- c(w[x < xstar[i]], wstar[i])
  wwarp_star[i] <- sum(abs(diff(w_combo)))
}

par(mfrow = c(1, 2))
plot(x, w, type = "b", main = "Original space, kriging map")
lines(xstar, wstar, col = 2, pch = 17)
plot(x, wwarp, type = "b", main = "Warped space, linear map")
lines(xstar, wwarp_star, col = 2, pch = 17)
```

This idea doesn't work, because things are not monotonic.  This was exactly the reasoning for the reference grid.  

Similarly, if we tried to krig directly to the warped space, this wouldn't enforce monotonicity.  And we wouldn't know the theta/tau2 to use.

Let's do the easy thing - linear mappings.

First, how did we previously deal with predictive locations that were outside of the training data?  This is inside the `fo_approx` function. The reference process anchors the edges of the space.

```{r}
xnew <- as.matrix(seq(0.1, 0.9, length = 10))
xgrid <- as.matrix(seq(0, 1, length = 50))
wgrid <- drop(rmvnorm(1, sigma = exp(-sq_dist(xgrid)/0.1)))
grid_index <- deepgp:::fo_approx_init(xgrid, xgrid)
wwarp <- deepgp:::monowarp_ref(xgrid, xgrid, wgrid, grid_index)
grid_index_new <- deepgp:::fo_approx_init(xgrid, xnew)
wnew <- deepgp:::monowarp_ref(xnew, xgrid, wgrid, grid_index_new)

plot(xgrid, wwarp, type = "b")
points(xnew, wnew, col = 2, pch = 17)
```

Would it have been ok if we skipped over it?  Yes, but the warping looks totally different.

```{r}
x <- as.matrix(sort(runif(20)))
w <- drop(rmvnorm(1, sigma = exp(-sq_dist(x)/0.1)))
xnew <- as.matrix(seq(0, 1, length = 10))
grid_index <- deepgp:::fo_approx_init(x, x)
wwarp <- deepgp:::monowarp_ref(x, x, w, grid_index)
grid_index_new <- deepgp:::fo_approx_init(x, xnew)
wnew <- deepgp:::monowarp_ref(xnew, x, w, grid_index_new)

plot(x, wwarp, type = "b", xlim = c(0, 1))
points(xnew, wnew, col = 2, pch = 17)
```

Can we do this in higher dimensions?

```{r}
x <- randomLHS(50, 2)
w <- rmvnorm(1, sigma = exp(-sq_dist(x)/0.1))

# Order by distance from the origin (x min values must be zero)
origin <- matrix(c(0, 0), nrow = 1)
d <- drop(sq_dist(origin, x))
ord <- order(d, decreasing = FALSE)
xord <- x[ord, ]
wwarp <- c(0, cumsum(abs(diff(w[ord]))))

par(mfrow = c(2, 2))
image(interp(x[, 1], x[, 2], w))
persp(interp(x[, 1], x[, 2], w), theta = 30, r = 30, phi = 30)
image(interp(xord[, 1], xord[, 2], wwarp))
persp(interp(xord[, 1], xord[, 2], wwarp), theta = 30, r = 30, phi = 30)
```

Yes, it works!  A reference process in higher dimensions is too much, but we can simply use the training data locations.  Then to predict, we do a linear interploation.

How does linear interpolation work in higher dimension?  We could use the Delaunay triangulation, then use linear interpolation of each triangle containing a predictive location.  In two dimensions, this is known as bilateral interpolation.  There are several R packages that offer this functionality.  In three dimensions, this is known as trilateral interpolation.  I have been unable to find any existing code for that.

The general idea is that the interpolated y value at a point in a "triangle" is a weighted average
of the values from the "triangle's" vertices (here I am using the term loosely since triangles are only the two dimensional form).

Key things I've learned:

* We can do a Delaunay triangulation with the `delaunayn` function in the `geometry` package.
* For a new location, we can identify the "triangle" that contains it with the `tsearchn` function.
* Then, how can we get the proper weights?  Barycentric coordinates...
* There is no clear way to extrapolate outside the convex hull.  I thus propose adding all corner points to
our proposals (even though they will not feature in the likelihood).  This will only get clunky in higher dimensions.

Example of adding corner points and finding the containing elements:

```{r}
library(geometry)

get_corners <- function(d) {
  bounds <- rep(list(c(0, 1)), times = d)
  return(as.matrix(expand.grid(bounds)))
}

d <- 2
x <- randomLHS(10, d)
x <- rbind(x, get_corners(d))
y <- x[, 1] + x[, 2]

xstar <- matrix(runif(d * 5), ncol = d)

# Triangulation
tri <- delaunayn(x, options = "Q12")

# Identify container
containers <- tsearchn(x, tri, xstar)
```

Now, how do we get the interpolation?

```{r}
# 2d implementation from interp package
if (d == 2) interp::interp(xvertices[, 1], xvertices[, 2], yvertices,
               xo = xstar[1], yo = xstar[2])$z

# My own way using barycentric coordinates (it matches for d = 2)
for (i in 1:nrow(xstar)) {
  xvertices <- x[tri[containers$idx[i], ], ]
  yvertices <- y[tri[containers$idx[i], ]]
  cat(sum(cart2bary(xvertices, xstar[i, , drop = FALSE]) * yvertices), "\n")
}
for (i in 1:nrow(xstar)) {
  yvertices <- y[tri[containers$idx[i], ]]
  cat(sum(containers$p[i, ] * yvertices), "\n")
}

# All together
yvertices <- matrix(y[tri[containers$idx, ]], ncol = d+1, byrow = FALSE)
rowSums(containers$p * yvertices)

xstar[1] + xstar[2]
```

```{r}
ord_from_origin <- function(x) {
  if (!is.matrix(x)) x <- as.matrix(x)
  d <- ncol(x)
  origin <- matrix(rep(0, times = d), nrow = 1)
  dist_from_origin <- drop(sq_dist(origin, x))
  return(order(dist_from_origin, decreasing = FALSE))
}

monotransform <- function(w, ord) {
  if (!is.matrix(w)) w <- as.matrix(w)
  D <- ncol(w)
  wwarp <- matrix(nrow = nrow(w), ncol = D)
  for (i in 1:D) {
    if (is.matrix(ord)) {
      ord_to_use <- ord[, i] # ordering based only on dimension i
    } else ord_to_use <- ord # single ordering for all inputs together
    wwarp_ordered <- c(0, cumsum(abs(diff(w[ord_to_use, i]))))
    wwarp[, i] <- wwarp_ordered[order(ord_to_use)]
  }
  return(wwarp)
}

### Test monotransform "together", 2 nodes of W
d <- 2
x <- randomLHS(50, d)
bounds <- rep(list(c(0, 1), times = d))
corners <- as.matrix(expand.grid(bounds))
x <- rbind(x, corners)
ord <- ord_from_origin(x)
w_orig <- t(rmvnorm(2, sigma = exp(-sq_dist(x)/0.1)))
w <- monotransform(w_orig, ord)

# monotonic across all dimensions
par(mfrow = c(2, 2))
for (i in 1:d) {
  image(interp(x[, 1], x[, 2], w_orig[, i]))
  image(interp(x[, 1], x[, 2], w[, i]))
}

# not guaranteed monotonic across 1d projections
for (i in 1:d) {
  plot(x[, 1], w_orig[, i])
  plot(x[, 2], w_orig[, i])
  plot(x[, 1], w[, i])
  plot(x[, 2], w[, i])
}

### Test monotransform "axis-aligned"
d <- 2
x <- randomLHS(50, d)
x <- rbind(x, rep(0, times = d), rep(1, times = d))
ord <- matrix(nrow = nrow(x), ncol = d)
for (i in 1:d) ord[, i] <- ord_from_origin(x[, i])
w_orig <- t(rmvnorm(2, sigma = exp(-sq_dist(x)/0.1)))
w <- monotransform(w, ord)

# not monotonic across all dimensions
par(mfrow = c(2, 2))
for (i in 1:d) {
  image(interp(x[, 1], x[, 2], w_orig[, i]))
  image(interp(x[, 1], x[, 2], w[, i]))
}

# but guaranteed monotonic across 1d projections
for (i in 1:d) {
  plot(x[, 1], w_orig[, i])
  plot(x[, 2], w_orig[, i])
  plot(x[, 1], w[, i])
  plot(x[, 2], w[, i])
}
```

