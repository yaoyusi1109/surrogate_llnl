---
title: "Variable Section with (D)GPs"
bibliography: ../ref.bib 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

# Previous works on variable selection for GPs

Most works on variable selection with GPs use a linear mean and a separable covariance with the lengthscale in the numerator.  

$$
\mathbf{y} \sim \mathcal{N}\left(X\beta, \Sigma_x\right)
\quad\textrm{where}\quad
\begin{array}{rl}
(X\beta)^{(i)} &= \sum_{d=1}^D x_{id}\beta_d \\
\Sigma_x^{(ij)} &= \tau^2\left[\mathrm{exp}\left(-\sum_{d=1}^D \theta_d(x_{id} - x_{jd})^2\right) + g\mathbb{I}_{i=j}\right]
\end{array}
$$

There is a closed-form MLE available for $\hat{\beta}$ and $\hat{\tau}^2$:

$$
\hat{\beta} = (X^\top \Sigma_x^{-1}X)^{-1} X^\top\Sigma_x^{-1} \mathbf{y}
$$

$$
\hat{\tau}^2 = \frac{1}{n}(\mathbf{y} - X\hat{\beta})^\top \Sigma_x^{-1}(\mathbf{y} - X\hat{\beta})
$$

The lengthscales $\boldsymbol\theta = [\theta_1, ..., \theta_d]$ requires numerical optimization.

In this framework, there are two avenues for "de-selecting" an input.

1. A particular $x_d$ could be taken out of the mean term $X\beta$.  This is equivalent to $\beta_d = 0$.
2. A particular $x_d$ could be taken out of the covariance term $\Sigma_n$.  This is equivalent to $\theta_d = 0$.

**Important Citations:**

* @linkletter2006variable: Places a mixture model prior on the lengthscale parameters with a point mass on zero.  Call the method "reference distribution variable selection."
* @joseph2008blind: Perform variable selection only in the $X\beta$ mean term.  Call this "blind kriging."
* @zhang2023indicator: Uses indicator functions to turn variables on/off.  If the indicator is zero, then the $\beta_i$ parameter in the $X\beta$ mean AND the $\theta_i$ parameter in the separable covariance are zero.  Note, to set $\theta$ equal to zero, we must put it in the numerator of the kernel.

# Traditional Monowarped DGPs

In our typical DGP framework, we have

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 K_{\theta_w}(W) + g\mathbb{I}\right) \\
\mathbf{w}_p &\sim\mathrm{GP}\mathcal{N}\left(0,\; K_{\theta_x^p}(X) \right) \\
&\quad p=1,\dots,P
\end{array}
\quad\textrm{where}\quad
\begin{array}{rl}
K_{\theta_w}(W)^{(ij)} &= \mathrm{exp}\left(-\frac{1}{\theta_w} \sum_{p=1}^P(w_{ip} - w_{jp})^2\right) \\
K_{\theta_x^p}(X)^{(ij)} &= \mathrm{exp}\left(-\frac{1}{\theta_x^p} \sum_{d=1}^D(x_{id} - x_{jd})^2\right)
\end{array}
$$

![](diagrams/Slide1.png)


In this original formulation, all dimensions of $X$ go to all dimensions of $W$.  There is no clear way to "de-select" a dimension of $X$.  If we wanted to draw inspiration from @linkletter2006variable, we could turn  $\theta_x^p$ into separable lengthscales, and then turn some of them off (going to $\infty$ in the denominator is equivalent to 0 in the numerator).  I think this is a terrible idea in our setting, because we would have $D\times P$ many unique lengthscale parameters to estimate here.  Very messy for very little gain.  [Or perhaps we give each dimension of $X$ a unique lengthscale, but keep these the same for all nodes of $W$?.  This just seems counterintuitive to me.]

Instead, I propose we embrace one-dimensional warpings.  Originally, we proposed the following monowarp DGP structure:

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 K_{\theta_w^d}(W) + g\mathbb{I}\right) \\
\mathbf{w}_d &\sim\mathrm{monoGP}\left(0,\; K_{\theta_x^d}(\mathbf{x}_d) \right) \quad d=1,\dots,D
\end{array}
$$
where kernels $K(\cdot)$ are defined as above, and each $\mathbf{w}_d$ is forced to be monotonic.  

![](diagrams/Slide2.png)


I have since developed a work-around to avoid separable lengthscales on the outer layer (by removing forced scaling of each warping).  We can then have the simpler model:

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau^2 K_{\theta_w}(W) + g\mathbb{I}\right) \\
\mathbf{w}_d &\sim\mathrm{monoGP}\left(0,\; K_{\theta_x^d}(\mathbf{x}_d) \right) \quad d=1,\dots,D
\end{array}
$$

![](diagrams/Slide3.png)

# Swapping $\tau^2$ and $\theta$

Notice, in the latent layer of the monowarp DGP, we have essentially fixed $\tau^2 = 1$ and allowed estimation of $\theta$.  Now we propose a SWAP.  Fix $\theta_x^d = 1$ for all $d=1,\dots,D$, but incorporate a $\tau^2_d$ parameter that may be estimated.  Our model is then:

$$
\begin{array}{rl}
\mathbf{y}&\sim\mathrm{GP}\left(0,\; \tau_w^2 K_{\theta_w}(W) + g\mathbb{I}\right) \\
\mathbf{w}_d &\sim\mathrm{monoGP}\left(0,\; \tau_{x_d}^2 K(\mathbf{x}_d) \right) \quad d=1,\dots,D
\end{array}
$$

Why is this so neat?  First, I have read [@zhang2004inconsistent] that in a one-dimensional (or isotropic) setting, only the ratio $\frac{\tau^2}{\theta}$ is identifiable.  The likelihood surface for these two variables is not conducive to optimizing.

### Let's investigate

To see this in action, let's create some random one-dimensional GP data and visualize the likelihood as a function of $\tau^2$ and $\theta$.

```{r, fig.width = 5, fig.height = 4}
library(deepgp)
library(mvtnorm)

x <- seq(0, 1, length = 10)
dx <- sq_dist(x)
y <- drop(rmvnorm(1, sigma = 1*exp(-dx/0.1)))
y <- (y - mean(y))/sd(y) # scale to zero mean unit variance

plot(x, y, main = "Toy data")
```

```{r, fig.width = 5, fig.height = 4}
theta <- round(exp(seq(log(0.01), log(1), length = 50)), 3)
tau2 <- round(seq(0.01, 10, length = 50), 3)
logl <- matrix(nrow = length(theta), ncol = length(tau2))
for (i in 1:length(theta))
  for (j in 1:length(tau2))
    logl[i, j] <- deepgp:::logl(y, dx, tau2 = tau2[j], theta = theta[i], 
                                g = 1e-6, v = 2.5)$ll

image(theta, tau2, logl, col = heat.colors(128), main = "logl")
contour(theta, tau2, logl, add = TRUE)
```

Certainly this surface is not conducive to numerical optimization.  Point made.

But, if we fix a value of $\tau^2$, we can perform effective MLE on $\theta$.

```{r, fig.width = 10}
theta <- round(exp(seq(log(0.01), log(1), length = 50)), 3)
tau2 <- c(0.1, 1, 10)
logl <- matrix(nrow = length(theta), ncol = length(tau2))
for (i in 1:length(theta))
  for (j in 1:length(tau2))
      logl[i, j] <- deepgp:::logl(y, dx, tau2 = tau2[j], theta = theta[i], 
                                g = 1e-6, v = 2.5)$ll

par(mfrow = c(1, 3))
for (j in 1:length(tau2)) {
  plot(theta, logl[, j], type = "l", main = paste0("tau2 = ", tau2[j]))
  points(theta[which.max(logl[, j])], max(logl[, j]), pch = 17, col = 4, cex = 2)
}

theta_for_tau2_1 <- theta[which.max(logl[, 2])] # save for later
```

We get different vales for the MLE of $\theta$, but these problems are all identifiable.

Can we do the same thing swapped?  Fix $\theta$ and estimate $\tau^2$?

```{r, fig.width = 10}
theta <- c(0.01, 0.1, 1)
tau2 <- round(seq(0.1, 10, length = 50), 3)
logl <- matrix(nrow = length(theta), ncol = length(tau2))
for (i in 1:length(theta))
  for (j in 1:length(tau2))
      logl[i, j] <- deepgp:::logl(y, dx, tau2 = tau2[j], theta = theta[i], 
                                g = 1e-6, v = 2.5)$ll

par(mfrow = c(1, 3))
for (i in 1:length(theta)) {
  plot(tau2, logl[i, ], type = "l", main = paste0("theta = ", theta[i]))
  points(tau2[which.max(logl[i, ])], max(logl[i, ]), pch = 17, col = 4, cex = 2)
}

tau2_for_theta_0.01 <- tau2[which.max(logl[, 1])] # save for later
```

So MLE is better behaved when we only estimate one of these parameters.  But how would this affect our statistical inferences?  Let's compare predictive surfaces.

```{r, fig.width = 10}
x_new <- seq(0, 1, length = 100)
dx_new <- sq_dist(x_new)
dx_cross <- sq_dist(x_new, x)

# First for tau2 = 1
p1 <- deepgp:::krig(y, dx, dx_new = dx_new, dx_cross = dx_cross, tau2 = 1, 
                   theta = theta_for_tau2_1, g = 1e-6, v = 2.5, mean = TRUE, s2 = TRUE)

# Second, fix theta = 0.1
p2 <- deepgp:::krig(y, dx, dx_new = dx_new, dx_cross = dx_cross, tau2 = tau2_for_theta_0.01, 
                   theta = 0.1, g = 1e-6, v = 2.5, mean = TRUE, s2 = TRUE)

par(mfrow = c(1, 2))
plot(x_new, p1$mean, type = "l", main = paste0("tau2 = 1, theta = ", theta_for_tau2_1))
lines(x_new, p1$mean - 2*sqrt(p1$s2), lty = 2)
lines(x_new, p1$mean + 2*sqrt(p1$s2), lty = 2)
points(x, y, col = 2)
plot(x_new, p2$mean, type = "l", main = paste0("tau2 = ", tau2_for_theta_0.01, ", theta = 0.1"))
lines(x_new, p2$mean - 2*sqrt(p2$s2), lty = 2)
lines(x_new, p2$mean + 2*sqrt(p2$s2), lty = 2)
points(x, y, pch = 20, col = 2)
```

Not much difference.  But things could go really poorly if $\theta$ is fixed too large.

### How do these values affect a monowarping?

We want to be confident that fixing $\theta$ and estimating $\tau^2$ will give us the same flexibility in our monotonic warpings.  We need to be cautious of fixing $\theta$ too large.

```{r}


```





-------
# References